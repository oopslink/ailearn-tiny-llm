# Tiny-LLM Learning Progress Tracker

**Project**: Building a simplified vLLM from scratch  
**Course**: [tiny-llm](https://skyzh.github.io/tiny-llm)  
**Start Date**: [Fill in your start date]  
**Target Completion**: [Fill in your target date]

---

## ğŸ“Š Overall Progress

| Phase | Topics | Status | Mastery |
|-------|--------|--------|---------|
| Week 1 | 7 days | 0/7 completed | 0% |
| Week 2 | 8 topics | 0/8 completed | 0% |
| Week 3 | 6 topics | 0/6 completed | 0% |
| **Total** | **21 topics** | **0/21** | **0%** |

**Last Updated**: [Auto-update with each session]

---

## ğŸ¯ Week 1: From Matmul to Text

Building the complete Qwen2 model from scratch using only matrix operations.

| Day | Topic | Status | Hours | Mastery | Notes |
|-----|-------|--------|-------|---------|-------|
| 1.1 | Attention & Multi-Head Attention | â¬œ Not Started | 0 | - | |
| 1.2 | Positional Encodings & RoPE | â¬œ Not Started | 0 | - | |
| 1.3 | Grouped/Multi Query Attention | â¬œ Not Started | 0 | - | |
| 1.4 | RMSNorm & MLP | â¬œ Not Started | 0 | - | |
| 1.5 | The Qwen2 Model | â¬œ Not Started | 0 | - | |
| 1.6 | Generating the Response | â¬œ Not Started | 0 | - | |
| 1.7 | Sampling & Week 2 Prep | â¬œ Not Started | 0 | - | |

**Week 1 Subtotal**: 0/7 topics completed, 0 hours invested

### Week 1 Key Concepts Status

| Concept | Understanding | Can Implement | Can Explain | Can Debug |
|---------|---------------|---------------|-------------|-----------|
| Scaled Dot Product Attention | â¬œ | â¬œ | â¬œ | â¬œ |
| Multi-Head Attention | â¬œ | â¬œ | â¬œ | â¬œ |
| Rotary Position Embeddings | â¬œ | â¬œ | â¬œ | â¬œ |
| Grouped Query Attention | â¬œ | â¬œ | â¬œ | â¬œ |
| RMSNorm | â¬œ | â¬œ | â¬œ | â¬œ |
| MLP/Feed-Forward | â¬œ | â¬œ | â¬œ | â¬œ |
| Token Generation | â¬œ | â¬œ | â¬œ | â¬œ |
| Sampling Strategies | â¬œ | â¬œ | â¬œ | â¬œ |

---

## ğŸš€ Week 2: Tiny vLLM

Implementing efficient inference infrastructure and optimizations.

| Topic | Component | Status | Hours | Mastery | Notes |
|-------|-----------|--------|-------|---------|-------|
| 2.1 | Key-Value Cache | â¬œ Not Started | 0 | - | |
| 2.2 | Quantized Matmul (CPU) | â¬œ Not Started | 0 | - | |
| 2.3 | Quantized Matmul (GPU/Metal) | â¬œ Not Started | 0 | - | |
| 2.4 | Flash Attention Theory | â¬œ Not Started | 0 | - | |
| 2.5 | Flash Attention Implementation | â¬œ Not Started | 0 | - | |
| 2.6 | Continuous Batching Theory | â¬œ Not Started | 0 | - | |
| 2.7 | Continuous Batching Implementation | â¬œ Not Started | 0 | - | |
| 2.8 | Chunked Prefill | â¬œ Not Started | 0 | - | |

**Week 2 Subtotal**: 0/8 topics completed, 0 hours invested

### Week 2 Key Concepts Status

| Concept | Understanding | Can Implement | Can Explain | Can Debug |
|---------|---------------|---------------|-------------|-----------|
| KV Cache Design | â¬œ | â¬œ | â¬œ | â¬œ |
| Matrix Quantization | â¬œ | â¬œ | â¬œ | â¬œ |
| C++ Kernel Programming | â¬œ | â¬œ | â¬œ | â¬œ |
| Metal Shader Programming | â¬œ | â¬œ | â¬œ | â¬œ |
| Flash Attention Algorithm | â¬œ | â¬œ | â¬œ | â¬œ |
| Memory-Efficient Attention | â¬œ | â¬œ | â¬œ | â¬œ |
| Request Batching | â¬œ | â¬œ | â¬œ | â¬œ |
| Dynamic Scheduling | â¬œ | â¬œ | â¬œ | â¬œ |

---

## ğŸŒŸ Week 3: Advanced Serving

Production-ready serving techniques (content TBD as course develops).

| Topic | Component | Status | Hours | Mastery | Notes |
|-------|-----------|--------|-------|---------|-------|
| 3.1 | Paged Attention | â¬œ Not Started | 0 | - | |
| 3.2 | Mixture of Experts | â¬œ Not Started | 0 | - | |
| 3.3 | Speculative Decoding | â¬œ Not Started | 0 | - | |
| 3.4 | RAG Pipeline | â¬œ Not Started | 0 | - | |
| 3.5 | Tool Calling/AI Agent | â¬œ Not Started | 0 | - | |
| 3.6 | Long Context Handling | â¬œ Not Started | 0 | - | |

**Week 3 Subtotal**: 0/6 topics completed, 0 hours invested

### Week 3 Key Concepts Status

| Concept | Understanding | Can Implement | Can Explain | Can Debug |
|---------|---------------|---------------|-------------|-----------|
| Paged Memory Management | â¬œ | â¬œ | â¬œ | â¬œ |
| MoE Architecture | â¬œ | â¬œ | â¬œ | â¬œ |
| Speculative Decoding | â¬œ | â¬œ | â¬œ | â¬œ |
| RAG Systems | â¬œ | â¬œ | â¬œ | â¬œ |
| Function Calling | â¬œ | â¬œ | â¬œ | â¬œ |
| Long Context Optimization | â¬œ | â¬œ | â¬œ | â¬œ |

---

## ğŸ’¡ Knowledge Gaps & Focus Areas

### Current Gaps
- [Document gaps as you discover them]

### Priority Learning Areas
1. [What needs more focus]
2. [What to review]
3. [What to practice]

### Concepts Needing Review
- [ ] [Topic to revisit]

---

## ğŸ¯ Milestones & Achievements

### Completed Milestones
- [ ] Environment setup complete
- [ ] First attention implementation working
- [ ] Complete Qwen2 model running
- [ ] First text generation successful
- [ ] KV cache implemented
- [ ] Custom kernel working
- [ ] Flash attention implemented
- [ ] Continuous batching working
- [ ] Week 3 advanced features complete

### Key Achievements
- [Document significant breakthroughs]

---

## ğŸ“ˆ Study Statistics

### Time Investment
- **Total Hours**: 0
- **Week 1**: 0 hours
- **Week 2**: 0 hours  
- **Week 3**: 0 hours
- **Average per topic**: 0 hours

### Session Summary
- **Total Sessions**: 0
- **Longest Session**: 0 hours
- **Most Productive Day**: [TBD]

### Struggle Points (> 2 hours on single topic)
- [Track difficult topics]

---

## ğŸš§ Current Status

**Phase**: Setup  
**Current Focus**: Environment preparation  
**Next Session**: Week 1 Day 1.1 - Attention Mechanisms

### This Week's Goals
- [ ] [Set weekly objectives]

### Blockers
- [None currently]

### Questions for Next Session
- [Prepare questions ahead]

---

## ğŸ“ Learning Insights

### What's Working Well
- [Reflect on effective strategies]

### What Needs Adjustment
- [Identify improvements needed]

### Key Insights Gained
- [Major "aha" moments]

---

## ğŸ”„ Weekly Reviews

### Week 1 Review (After completion)
- **Topics mastered**: 
- **Time invested**: 
- **Key learnings**:
- **Challenges overcome**:
- **Confidence level**: /10

### Week 2 Review (After completion)
- **Topics mastered**: 
- **Time invested**: 
- **Key learnings**:
- **Challenges overcome**:
- **Confidence level**: /10

### Week 3 Review (After completion)
- **Topics mastered**: 
- **Time invested**: 
- **Key learnings**:
- **Challenges overcome**:
- **Confidence level**: /10

---

## ğŸ“ Final Assessment

### Overall Learning Outcomes
- [ ] Can implement LLM inference from scratch
- [ ] Understand attention mechanisms deeply
- [ ] Can write custom CUDA/Metal kernels
- [ ] Understand serving system tradeoffs
- [ ] Can debug complex ML systems
- [ ] Can optimize for performance
- [ ] Ready to read vLLM source code
- [ ] Can contribute to LLM serving projects

### Skills Acquired
- [List new capabilities]

### Next Steps After Course
- [Future learning goals]

---

**Legend**:
- â¬œ Not Started
- ğŸŸ¡ In Progress
- âœ… Completed
- â­ Mastered

**Mastery Levels**:
- **-**: Not started
- **ğŸ“– Learning**: Understanding concepts
- **ğŸ’» Implementing**: Can code with reference
- **âœ… Competent**: Can implement independently
- **â­ Mastery**: Can explain, debug, and extend
