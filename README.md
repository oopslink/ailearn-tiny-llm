# Tiny-LLM Learning Project

A structured learning project for the [tiny-llm course](https://skyzh.github.io/tiny-llm/) - learning LLM inference serving on Apple Silicon for systems engineers.

## About

This repository documents my journey through the tiny-llm course, which focuses on building a tiny vLLM + Qwen implementation. The course teaches practical skills in LLM inference optimization and serving, specifically optimized for Apple Silicon.

## Project Structure

```
.
├── chapters/           # Chapter-by-chapter learning materials
│   ├── ch01-intro/
│   ├── ch02-tokenization/
│   ├── ch03-inference/
│   ├── ch04-optimization/
│   └── ch05-serving/
├── notes/             # General learning notes and summaries
├── code/              # Code implementations and experiments
├── exercises/         # Exercise solutions and practice projects
└── assets/            # Images, diagrams, and other resources
```

## Learning Resources

- **Course Website**: https://skyzh.github.io/tiny-llm/
- **Course Repository**: https://github.com/skyzh/tiny-llm

## Prerequisites

- Basic understanding of Python
- Familiarity with machine learning concepts
- Apple Silicon Mac (M1/M2/M3) for optimal performance
- Python 3.8+

## Getting Started

1. Clone this repository:
   ```bash
   git clone <your-repo-url>
   cd ailearn-tiny-llm
   ```

2. Follow along with the course materials at https://skyzh.github.io/tiny-llm/

3. Document your learning in the respective chapter directories

4. Implement code examples in the `code/` directory

5. Complete exercises in the `exercises/` directory

## Course Topics

The tiny-llm course covers:

- **Introduction**: Fundamentals of LLM inference and serving
- **Tokenization**: Understanding and implementing tokenization strategies
- **Inference**: Building the inference pipeline
- **Optimization**: Performance and memory optimization for Apple Silicon
- **Serving**: Production-ready LLM serving infrastructure

## Progress Tracking

- [ ] Chapter 1: Introduction to LLM Inference
- [ ] Chapter 2: Tokenization
- [ ] Chapter 3: LLM Inference
- [ ] Chapter 4: Optimization Techniques
- [ ] Chapter 5: LLM Serving

## Contributing

This is a personal learning repository. Feel free to fork and adapt it for your own learning journey!

## License

This project is for educational purposes. Please refer to the original course materials for their respective licenses.

## Acknowledgments

This learning project is inspired by and structured with reference to:

- **[tiny-llm Course](https://github.com/skyzh/tiny-llm)** by [@skyzh](https://github.com/skyzh) - The comprehensive course on LLM inference serving on Apple Silicon that this project follows. The course provides excellent hands-on learning materials for systems engineers interested in LLM optimization and serving.

- **[CFP-Study](https://github.com/chenran818/CFP-study)** by [@chenran818](https://github.com/chenran818) - Referenced as a template for organizing course learning materials. This project's structure and organization approach draws inspiration from the CFP-study repository's systematic way of documenting learning progress.

Special thanks to these projects for making quality learning resources available to the community!

## Contact

For questions about the course content, please refer to the official [tiny-llm repository](https://github.com/skyzh/tiny-llm).
